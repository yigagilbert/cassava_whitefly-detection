{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yigagilbert/cassava_whitefly-detection/blob/main/whitefly_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRYp0mtxRXlF"
      },
      "outputs": [],
      "source": [
        "from numpy.random import seed\n",
        "seed(1)\n",
        "import os\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, Conv3D,BatchNormalization\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from time import strftime, localtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcRrsUtTRg1m"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANQ0zg2fRla3"
      },
      "source": [
        "**MODEL** **DEF**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gluAmmJkRj3a"
      },
      "outputs": [],
      "source": [
        "# Instatiate model\n",
        "model = Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_iIT5rYRxF7"
      },
      "outputs": [],
      "source": [
        "model.add(Conv2D(512, kernel_size=(5, 5), input_shape=(25, 25, 3), activation='relu', padding='same'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(256, kernel_size=(5,5), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Conv2D(128, kernel_size=(3,3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Flatten())\n",
        "# # model.add(Dense(8, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "#model.add(BatchNormalization())\n",
        "model.add(Dropout(0.33))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9QpsmKURyK5",
        "outputId": "253e67bb-b4d7-478d-883f-ae99e4c6839b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 25, 25, 512)       38912     \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 12, 12, 512)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 8, 8, 256)         3277056   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 8, 8, 256)        1024      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 4, 4, 256)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 2, 2, 128)         295040    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 2, 2, 128)        512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 1, 1, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8)                 1032      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8)                 0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,613,585\n",
            "Trainable params: 3,612,817\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YWeBfwjR4LA"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=SGD(learning_rate=0.0001), loss = 'binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tbsxnjJnR76l",
        "outputId": "bacb7d44-7a7a-4a11-87a9-4bf50f8e2322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'weights/07-07-2022_09:54:06_weights.{epoch:02d}.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "paths = \"weights/\"+strftime(\"%m-%d-%Y_%H:%M:%S\")+\"_weights.{epoch:02d}.h5\"\n",
        "paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQU51Gk-R-0J"
      },
      "outputs": [],
      "source": [
        "# paths = \"weights_25/weights.{epoch:02d}.h5\"\n",
        "monitor_progress = ModelCheckpoint(paths,monitor=\"val_accuracy\", verbose=1, save_best_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSCtNuRxSABn"
      },
      "source": [
        "**DATA** **PRE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXkZ49R3S03N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4c48ec-a56b-4b38-b498-56452366fc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT2CI92zT8S1"
      },
      "outputs": [],
      "source": [
        "!unzip /content/gdrive/MyDrive/25by25.zip -d /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8PT_avVSJon"
      },
      "outputs": [],
      "source": [
        "base_dir ='/content/sample_data/25by25'  \n",
        "traindir = os.path.join(base_dir,\"train\")\n",
        "validationdir = os.path.join(base_dir,\"val\")\n",
        "testdir = os.path.join(base_dir,\"test\") \n",
        "patch_size=25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAJEK8opSME5"
      },
      "outputs": [],
      "source": [
        "traindatagen = ImageDataGenerator( rotation_range=40,\n",
        "        width_shift_range=0.4,\n",
        "        height_shift_range=0.4,\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.4,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest')\n",
        "validationdatagen = ImageDataGenerator(rescale=1/225.)\n",
        "testdatagen = ImageDataGenerator(rescale=1/225.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgfAbx2BSO0Y",
        "outputId": "36003d63-0172-48be-83d7-6a90c731fe63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2781 images belonging to 2 classes.\n",
            "Found 347 images belonging to 2 classes.\n",
            "Found 349 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "traingen = traindatagen.flow_from_directory(\n",
        "                               directory=traindir,\n",
        "                                target_size=(patch_size,patch_size),\n",
        "                                batch_size = 8,\n",
        "                                class_mode='binary')\n",
        "validationgen = validationdatagen.flow_from_directory(\n",
        "                               directory=validationdir,\n",
        "                                target_size=(patch_size,patch_size),\n",
        "                                batch_size = 8,\n",
        "                                class_mode='binary')\n",
        "testgen = testdatagen.flow_from_directory(\n",
        "                                directory=testdir,\n",
        "                                target_size=(patch_size,patch_size),\n",
        "                                batch_size = 8,                                \n",
        "                                class_mode='binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHIsjfuZSROx",
        "outputId": "cc5645c5-7bb6-487f-eae5-07862f760db4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'negative': 0, 'positive': 1}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "validationgen.class_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fnm7XTWST1v",
        "outputId": "7a27ebba-2640-4adb-d1ae-71fea16106e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "348"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "traingen.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCCq4VFwSWgX",
        "outputId": "16da29c6-8c6d-4e16-c43c-c5ddaff01975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.4621 - accuracy: 0.8273\n",
            "Epoch 1: val_accuracy improved from -inf to 0.90202, saving model to weights/07-07-2022_05:42:07_weights.01.h5\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.4613 - accuracy: 0.8278 - val_loss: 0.4505 - val_accuracy: 0.9020\n",
            "Epoch 2/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.3528 - accuracy: 0.8778\n",
            "Epoch 2: val_accuracy improved from 0.90202 to 0.92219, saving model to weights/07-07-2022_05:42:07_weights.02.h5\n",
            "348/348 [==============================] - 4s 12ms/step - loss: 0.3517 - accuracy: 0.8781 - val_loss: 0.2977 - val_accuracy: 0.9222\n",
            "Epoch 3/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.3381 - accuracy: 0.8759\n",
            "Epoch 3: val_accuracy did not improve from 0.92219\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.3373 - accuracy: 0.8763 - val_loss: 0.3672 - val_accuracy: 0.9164\n",
            "Epoch 4/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.3355 - accuracy: 0.8745\n",
            "Epoch 4: val_accuracy did not improve from 0.92219\n",
            "348/348 [==============================] - 4s 12ms/step - loss: 0.3345 - accuracy: 0.8752 - val_loss: 0.4162 - val_accuracy: 0.9049\n",
            "Epoch 5/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8767\n",
            "Epoch 5: val_accuracy did not improve from 0.92219\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.3152 - accuracy: 0.8763 - val_loss: 0.3174 - val_accuracy: 0.9107\n",
            "Epoch 6/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.8949\n",
            "Epoch 6: val_accuracy improved from 0.92219 to 0.92795, saving model to weights/07-07-2022_05:42:07_weights.06.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2804 - accuracy: 0.8957 - val_loss: 0.1929 - val_accuracy: 0.9280\n",
            "Epoch 7/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2785 - accuracy: 0.8937\n",
            "Epoch 7: val_accuracy did not improve from 0.92795\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2774 - accuracy: 0.8943 - val_loss: 0.2019 - val_accuracy: 0.9251\n",
            "Epoch 8/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2629 - accuracy: 0.9006\n",
            "Epoch 8: val_accuracy did not improve from 0.92795\n",
            "348/348 [==============================] - 4s 12ms/step - loss: 0.2626 - accuracy: 0.9008 - val_loss: 0.2416 - val_accuracy: 0.9078\n",
            "Epoch 9/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.2638 - accuracy: 0.9023\n",
            "Epoch 9: val_accuracy did not improve from 0.92795\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2641 - accuracy: 0.9026 - val_loss: 0.2101 - val_accuracy: 0.9222\n",
            "Epoch 10/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.2366 - accuracy: 0.9215\n",
            "Epoch 10: val_accuracy improved from 0.92795 to 0.93084, saving model to weights/07-07-2022_05:42:07_weights.10.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2376 - accuracy: 0.9205 - val_loss: 0.1841 - val_accuracy: 0.9308\n",
            "Epoch 11/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.2488 - accuracy: 0.9103\n",
            "Epoch 11: val_accuracy improved from 0.93084 to 0.94236, saving model to weights/07-07-2022_05:42:07_weights.11.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2492 - accuracy: 0.9101 - val_loss: 0.2540 - val_accuracy: 0.9424\n",
            "Epoch 12/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.2410 - accuracy: 0.9123\n",
            "Epoch 12: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2401 - accuracy: 0.9123 - val_loss: 0.3017 - val_accuracy: 0.9107\n",
            "Epoch 13/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.2282 - accuracy: 0.9239\n",
            "Epoch 13: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2276 - accuracy: 0.9241 - val_loss: 0.1724 - val_accuracy: 0.9193\n",
            "Epoch 14/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9264\n",
            "Epoch 14: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2233 - accuracy: 0.9263 - val_loss: 0.1860 - val_accuracy: 0.9222\n",
            "Epoch 15/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9241\n",
            "Epoch 15: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.2301 - accuracy: 0.9241 - val_loss: 0.1959 - val_accuracy: 0.9280\n",
            "Epoch 16/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.2207 - accuracy: 0.9232\n",
            "Epoch 16: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.2205 - accuracy: 0.9234 - val_loss: 0.1602 - val_accuracy: 0.9193\n",
            "Epoch 17/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9322\n",
            "Epoch 17: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2020 - accuracy: 0.9317 - val_loss: 0.1575 - val_accuracy: 0.9280\n",
            "Epoch 18/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9275\n",
            "Epoch 18: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2099 - accuracy: 0.9274 - val_loss: 0.1821 - val_accuracy: 0.9193\n",
            "Epoch 19/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.2113 - accuracy: 0.9306\n",
            "Epoch 19: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2113 - accuracy: 0.9306 - val_loss: 0.1942 - val_accuracy: 0.9135\n",
            "Epoch 20/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.2080 - accuracy: 0.9306\n",
            "Epoch 20: val_accuracy did not improve from 0.94236\n",
            "348/348 [==============================] - 4s 12ms/step - loss: 0.2094 - accuracy: 0.9299 - val_loss: 0.1489 - val_accuracy: 0.9193\n",
            "Epoch 21/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1973 - accuracy: 0.9306\n",
            "Epoch 21: val_accuracy improved from 0.94236 to 0.94813, saving model to weights/07-07-2022_05:42:07_weights.21.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1968 - accuracy: 0.9310 - val_loss: 0.1341 - val_accuracy: 0.9481\n",
            "Epoch 22/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.2087 - accuracy: 0.9300\n",
            "Epoch 22: val_accuracy did not improve from 0.94813\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2084 - accuracy: 0.9302 - val_loss: 0.1295 - val_accuracy: 0.9251\n",
            "Epoch 23/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.2077 - accuracy: 0.9349\n",
            "Epoch 23: val_accuracy did not improve from 0.94813\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2077 - accuracy: 0.9349 - val_loss: 0.1766 - val_accuracy: 0.9107\n",
            "Epoch 24/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9343\n",
            "Epoch 24: val_accuracy did not improve from 0.94813\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2076 - accuracy: 0.9335 - val_loss: 0.1934 - val_accuracy: 0.9164\n",
            "Epoch 25/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9392\n",
            "Epoch 25: val_accuracy did not improve from 0.94813\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1905 - accuracy: 0.9392 - val_loss: 0.1568 - val_accuracy: 0.9222\n",
            "Epoch 26/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.2027 - accuracy: 0.9371\n",
            "Epoch 26: val_accuracy did not improve from 0.94813\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2041 - accuracy: 0.9364 - val_loss: 0.1709 - val_accuracy: 0.9078\n",
            "Epoch 27/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.2023 - accuracy: 0.9296\n",
            "Epoch 27: val_accuracy did not improve from 0.94813\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.2017 - accuracy: 0.9299 - val_loss: 0.1402 - val_accuracy: 0.9308\n",
            "Epoch 28/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1974 - accuracy: 0.9376\n",
            "Epoch 28: val_accuracy improved from 0.94813 to 0.95101, saving model to weights/07-07-2022_05:42:07_weights.28.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1974 - accuracy: 0.9378 - val_loss: 0.1770 - val_accuracy: 0.9510\n",
            "Epoch 29/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1836 - accuracy: 0.9430\n",
            "Epoch 29: val_accuracy did not improve from 0.95101\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1832 - accuracy: 0.9432 - val_loss: 0.1379 - val_accuracy: 0.9222\n",
            "Epoch 30/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1755 - accuracy: 0.9462\n",
            "Epoch 30: val_accuracy did not improve from 0.95101\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1762 - accuracy: 0.9457 - val_loss: 0.1872 - val_accuracy: 0.9510\n",
            "Epoch 31/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1767 - accuracy: 0.9410\n",
            "Epoch 31: val_accuracy improved from 0.95101 to 0.97695, saving model to weights/07-07-2022_05:42:07_weights.31.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1769 - accuracy: 0.9410 - val_loss: 0.1667 - val_accuracy: 0.9769\n",
            "Epoch 32/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9364\n",
            "Epoch 32: val_accuracy did not improve from 0.97695\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1936 - accuracy: 0.9364 - val_loss: 0.1397 - val_accuracy: 0.9337\n",
            "Epoch 33/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1846 - accuracy: 0.9419\n",
            "Epoch 33: val_accuracy did not improve from 0.97695\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1843 - accuracy: 0.9421 - val_loss: 0.1589 - val_accuracy: 0.9683\n",
            "Epoch 34/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1743 - accuracy: 0.9468\n",
            "Epoch 34: val_accuracy did not improve from 0.97695\n",
            "348/348 [==============================] - 6s 16ms/step - loss: 0.1754 - accuracy: 0.9464 - val_loss: 0.1166 - val_accuracy: 0.9424\n",
            "Epoch 35/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1851 - accuracy: 0.9369\n",
            "Epoch 35: val_accuracy improved from 0.97695 to 0.98559, saving model to weights/07-07-2022_05:42:07_weights.35.h5\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1857 - accuracy: 0.9364 - val_loss: 0.1552 - val_accuracy: 0.9856\n",
            "Epoch 36/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1861 - accuracy: 0.9419\n",
            "Epoch 36: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1863 - accuracy: 0.9417 - val_loss: 0.1488 - val_accuracy: 0.9308\n",
            "Epoch 37/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1906 - accuracy: 0.9360\n",
            "Epoch 37: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1908 - accuracy: 0.9360 - val_loss: 0.1924 - val_accuracy: 0.9452\n",
            "Epoch 38/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1765 - accuracy: 0.9389\n",
            "Epoch 38: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1765 - accuracy: 0.9389 - val_loss: 0.1402 - val_accuracy: 0.9164\n",
            "Epoch 39/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1899 - accuracy: 0.9376\n",
            "Epoch 39: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1897 - accuracy: 0.9378 - val_loss: 0.1367 - val_accuracy: 0.9251\n",
            "Epoch 40/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1814 - accuracy: 0.9382\n",
            "Epoch 40: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1814 - accuracy: 0.9382 - val_loss: 0.1176 - val_accuracy: 0.9856\n",
            "Epoch 41/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9425\n",
            "Epoch 41: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1807 - accuracy: 0.9425 - val_loss: 0.1081 - val_accuracy: 0.9510\n",
            "Epoch 42/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1811 - accuracy: 0.9463\n",
            "Epoch 42: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1807 - accuracy: 0.9464 - val_loss: 0.1090 - val_accuracy: 0.9395\n",
            "Epoch 43/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1729 - accuracy: 0.9449\n",
            "Epoch 43: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1728 - accuracy: 0.9450 - val_loss: 0.0995 - val_accuracy: 0.9798\n",
            "Epoch 44/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1790 - accuracy: 0.9440\n",
            "Epoch 44: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1803 - accuracy: 0.9435 - val_loss: 0.1679 - val_accuracy: 0.9193\n",
            "Epoch 45/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1650 - accuracy: 0.9525\n",
            "Epoch 45: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1653 - accuracy: 0.9525 - val_loss: 0.1104 - val_accuracy: 0.9798\n",
            "Epoch 46/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9403\n",
            "Epoch 46: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1847 - accuracy: 0.9403 - val_loss: 0.1222 - val_accuracy: 0.9539\n",
            "Epoch 47/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1614 - accuracy: 0.9542\n",
            "Epoch 47: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1623 - accuracy: 0.9533 - val_loss: 0.1201 - val_accuracy: 0.9597\n",
            "Epoch 48/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9446\n",
            "Epoch 48: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1725 - accuracy: 0.9446 - val_loss: 0.1327 - val_accuracy: 0.9308\n",
            "Epoch 49/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9460\n",
            "Epoch 49: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1618 - accuracy: 0.9453 - val_loss: 0.1885 - val_accuracy: 0.9049\n",
            "Epoch 50/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9500\n",
            "Epoch 50: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1715 - accuracy: 0.9500 - val_loss: 0.1311 - val_accuracy: 0.9424\n",
            "Epoch 51/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1718 - accuracy: 0.9455\n",
            "Epoch 51: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1714 - accuracy: 0.9457 - val_loss: 0.1058 - val_accuracy: 0.9366\n",
            "Epoch 52/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1647 - accuracy: 0.9498\n",
            "Epoch 52: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1650 - accuracy: 0.9497 - val_loss: 0.1112 - val_accuracy: 0.9337\n",
            "Epoch 53/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1620 - accuracy: 0.9515\n",
            "Epoch 53: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1615 - accuracy: 0.9518 - val_loss: 0.1697 - val_accuracy: 0.9337\n",
            "Epoch 54/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1654 - accuracy: 0.9481\n",
            "Epoch 54: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1653 - accuracy: 0.9482 - val_loss: 0.1052 - val_accuracy: 0.9481\n",
            "Epoch 55/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1708 - accuracy: 0.9433\n",
            "Epoch 55: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1697 - accuracy: 0.9439 - val_loss: 0.1162 - val_accuracy: 0.9481\n",
            "Epoch 56/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1780 - accuracy: 0.9439\n",
            "Epoch 56: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1772 - accuracy: 0.9443 - val_loss: 0.1086 - val_accuracy: 0.9424\n",
            "Epoch 57/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1739 - accuracy: 0.9497\n",
            "Epoch 57: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1730 - accuracy: 0.9500 - val_loss: 0.0915 - val_accuracy: 0.9654\n",
            "Epoch 58/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1649 - accuracy: 0.9557\n",
            "Epoch 58: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1660 - accuracy: 0.9554 - val_loss: 0.1108 - val_accuracy: 0.9395\n",
            "Epoch 59/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1543 - accuracy: 0.9492\n",
            "Epoch 59: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1544 - accuracy: 0.9482 - val_loss: 0.0918 - val_accuracy: 0.9769\n",
            "Epoch 60/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1566 - accuracy: 0.9474\n",
            "Epoch 60: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1581 - accuracy: 0.9468 - val_loss: 0.1677 - val_accuracy: 0.9135\n",
            "Epoch 61/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9507\n",
            "Epoch 61: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1599 - accuracy: 0.9507 - val_loss: 0.1115 - val_accuracy: 0.9827\n",
            "Epoch 62/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1618 - accuracy: 0.9461\n",
            "Epoch 62: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 6s 17ms/step - loss: 0.1618 - accuracy: 0.9461 - val_loss: 0.1532 - val_accuracy: 0.9625\n",
            "Epoch 63/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9463\n",
            "Epoch 63: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 0.1694 - accuracy: 0.9450 - val_loss: 0.1069 - val_accuracy: 0.9654\n",
            "Epoch 64/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1639 - accuracy: 0.9509\n",
            "Epoch 64: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1636 - accuracy: 0.9511 - val_loss: 0.1620 - val_accuracy: 0.9510\n",
            "Epoch 65/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9470\n",
            "Epoch 65: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1679 - accuracy: 0.9471 - val_loss: 0.1129 - val_accuracy: 0.9366\n",
            "Epoch 66/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1576 - accuracy: 0.9503\n",
            "Epoch 66: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1574 - accuracy: 0.9504 - val_loss: 0.1118 - val_accuracy: 0.9712\n",
            "Epoch 67/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9511\n",
            "Epoch 67: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1564 - accuracy: 0.9511 - val_loss: 0.1207 - val_accuracy: 0.9251\n",
            "Epoch 68/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1660 - accuracy: 0.9480\n",
            "Epoch 68: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1657 - accuracy: 0.9479 - val_loss: 0.1204 - val_accuracy: 0.9424\n",
            "Epoch 69/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1628 - accuracy: 0.9491\n",
            "Epoch 69: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1614 - accuracy: 0.9497 - val_loss: 0.1190 - val_accuracy: 0.9251\n",
            "Epoch 70/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1671 - accuracy: 0.9470\n",
            "Epoch 70: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1671 - accuracy: 0.9468 - val_loss: 0.1703 - val_accuracy: 0.9193\n",
            "Epoch 71/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1546 - accuracy: 0.9489\n",
            "Epoch 71: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1551 - accuracy: 0.9489 - val_loss: 0.0912 - val_accuracy: 0.9510\n",
            "Epoch 72/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9573\n",
            "Epoch 72: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1462 - accuracy: 0.9576 - val_loss: 0.1366 - val_accuracy: 0.9308\n",
            "Epoch 73/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1572 - accuracy: 0.9534\n",
            "Epoch 73: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1593 - accuracy: 0.9522 - val_loss: 0.1363 - val_accuracy: 0.9395\n",
            "Epoch 74/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.9518\n",
            "Epoch 74: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 4s 13ms/step - loss: 0.1537 - accuracy: 0.9518 - val_loss: 0.1098 - val_accuracy: 0.9481\n",
            "Epoch 75/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1579 - accuracy: 0.9446\n",
            "Epoch 75: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1579 - accuracy: 0.9446 - val_loss: 0.1040 - val_accuracy: 0.9683\n",
            "Epoch 76/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1763 - accuracy: 0.9434\n",
            "Epoch 76: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1761 - accuracy: 0.9432 - val_loss: 0.1514 - val_accuracy: 0.9452\n",
            "Epoch 77/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1534 - accuracy: 0.9517\n",
            "Epoch 77: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1534 - accuracy: 0.9518 - val_loss: 0.1337 - val_accuracy: 0.9366\n",
            "Epoch 78/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1528 - accuracy: 0.9464\n",
            "Epoch 78: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1528 - accuracy: 0.9464 - val_loss: 0.0943 - val_accuracy: 0.9597\n",
            "Epoch 79/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1526 - accuracy: 0.9544\n",
            "Epoch 79: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1519 - accuracy: 0.9547 - val_loss: 0.1284 - val_accuracy: 0.9222\n",
            "Epoch 80/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1618 - accuracy: 0.9519\n",
            "Epoch 80: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1618 - accuracy: 0.9522 - val_loss: 0.0964 - val_accuracy: 0.9769\n",
            "Epoch 81/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.9474\n",
            "Epoch 81: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1532 - accuracy: 0.9479 - val_loss: 0.1021 - val_accuracy: 0.9337\n",
            "Epoch 82/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1583 - accuracy: 0.9502\n",
            "Epoch 82: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1579 - accuracy: 0.9504 - val_loss: 0.1176 - val_accuracy: 0.9481\n",
            "Epoch 83/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1408 - accuracy: 0.9570\n",
            "Epoch 83: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1410 - accuracy: 0.9572 - val_loss: 0.0961 - val_accuracy: 0.9597\n",
            "Epoch 84/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1544 - accuracy: 0.9528\n",
            "Epoch 84: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1540 - accuracy: 0.9529 - val_loss: 0.0987 - val_accuracy: 0.9452\n",
            "Epoch 85/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9560\n",
            "Epoch 85: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1411 - accuracy: 0.9565 - val_loss: 0.1684 - val_accuracy: 0.9395\n",
            "Epoch 86/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1499 - accuracy: 0.9541\n",
            "Epoch 86: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1498 - accuracy: 0.9540 - val_loss: 0.1599 - val_accuracy: 0.9597\n",
            "Epoch 87/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1322 - accuracy: 0.9549\n",
            "Epoch 87: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1325 - accuracy: 0.9551 - val_loss: 0.0970 - val_accuracy: 0.9741\n",
            "Epoch 88/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9497\n",
            "Epoch 88: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1596 - accuracy: 0.9497 - val_loss: 0.0929 - val_accuracy: 0.9798\n",
            "Epoch 89/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9510\n",
            "Epoch 89: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1436 - accuracy: 0.9511 - val_loss: 0.1203 - val_accuracy: 0.9625\n",
            "Epoch 90/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1555 - accuracy: 0.9506\n",
            "Epoch 90: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1554 - accuracy: 0.9507 - val_loss: 0.0949 - val_accuracy: 0.9539\n",
            "Epoch 91/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1497 - accuracy: 0.9550\n",
            "Epoch 91: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1500 - accuracy: 0.9547 - val_loss: 0.0866 - val_accuracy: 0.9683\n",
            "Epoch 92/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1535 - accuracy: 0.9548\n",
            "Epoch 92: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1537 - accuracy: 0.9547 - val_loss: 0.1147 - val_accuracy: 0.9625\n",
            "Epoch 93/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1383 - accuracy: 0.9554\n",
            "Epoch 93: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1383 - accuracy: 0.9554 - val_loss: 0.0906 - val_accuracy: 0.9741\n",
            "Epoch 94/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9555\n",
            "Epoch 94: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 0.1481 - accuracy: 0.9551 - val_loss: 0.1101 - val_accuracy: 0.9424\n",
            "Epoch 95/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9532\n",
            "Epoch 95: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1418 - accuracy: 0.9533 - val_loss: 0.1258 - val_accuracy: 0.9712\n",
            "Epoch 96/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1345 - accuracy: 0.9587\n",
            "Epoch 96: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1352 - accuracy: 0.9583 - val_loss: 0.1192 - val_accuracy: 0.9683\n",
            "Epoch 97/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1500 - accuracy: 0.9554\n",
            "Epoch 97: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1494 - accuracy: 0.9558 - val_loss: 0.1017 - val_accuracy: 0.9769\n",
            "Epoch 98/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1475 - accuracy: 0.9519\n",
            "Epoch 98: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1473 - accuracy: 0.9522 - val_loss: 0.1038 - val_accuracy: 0.9625\n",
            "Epoch 99/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9543\n",
            "Epoch 99: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1476 - accuracy: 0.9547 - val_loss: 0.1069 - val_accuracy: 0.9481\n",
            "Epoch 100/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1537 - accuracy: 0.9521\n",
            "Epoch 100: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1528 - accuracy: 0.9525 - val_loss: 0.1389 - val_accuracy: 0.9337\n",
            "Epoch 101/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9606\n",
            "Epoch 101: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1407 - accuracy: 0.9608 - val_loss: 0.0959 - val_accuracy: 0.9597\n",
            "Epoch 102/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9517\n",
            "Epoch 102: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1544 - accuracy: 0.9518 - val_loss: 0.0942 - val_accuracy: 0.9741\n",
            "Epoch 103/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9582\n",
            "Epoch 103: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1451 - accuracy: 0.9583 - val_loss: 0.1801 - val_accuracy: 0.9222\n",
            "Epoch 104/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.9542\n",
            "Epoch 104: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1416 - accuracy: 0.9547 - val_loss: 0.1019 - val_accuracy: 0.9741\n",
            "Epoch 105/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9572\n",
            "Epoch 105: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1397 - accuracy: 0.9572 - val_loss: 0.1505 - val_accuracy: 0.9510\n",
            "Epoch 106/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1436 - accuracy: 0.9633\n",
            "Epoch 106: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1436 - accuracy: 0.9633 - val_loss: 0.1025 - val_accuracy: 0.9539\n",
            "Epoch 107/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1398 - accuracy: 0.9580\n",
            "Epoch 107: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1405 - accuracy: 0.9576 - val_loss: 0.0895 - val_accuracy: 0.9625\n",
            "Epoch 108/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9541\n",
            "Epoch 108: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1436 - accuracy: 0.9543 - val_loss: 0.1446 - val_accuracy: 0.9395\n",
            "Epoch 109/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1427 - accuracy: 0.9549\n",
            "Epoch 109: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1417 - accuracy: 0.9551 - val_loss: 0.1090 - val_accuracy: 0.9510\n",
            "Epoch 110/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1343 - accuracy: 0.9584\n",
            "Epoch 110: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1350 - accuracy: 0.9579 - val_loss: 0.0897 - val_accuracy: 0.9827\n",
            "Epoch 111/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9502\n",
            "Epoch 111: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1371 - accuracy: 0.9497 - val_loss: 0.1066 - val_accuracy: 0.9683\n",
            "Epoch 112/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1331 - accuracy: 0.9583\n",
            "Epoch 112: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1330 - accuracy: 0.9583 - val_loss: 0.0920 - val_accuracy: 0.9712\n",
            "Epoch 113/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1418 - accuracy: 0.9538\n",
            "Epoch 113: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1417 - accuracy: 0.9540 - val_loss: 0.0898 - val_accuracy: 0.9827\n",
            "Epoch 114/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1376 - accuracy: 0.9560\n",
            "Epoch 114: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1375 - accuracy: 0.9561 - val_loss: 0.0951 - val_accuracy: 0.9597\n",
            "Epoch 115/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1464 - accuracy: 0.9553\n",
            "Epoch 115: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1457 - accuracy: 0.9554 - val_loss: 0.0824 - val_accuracy: 0.9625\n",
            "Epoch 116/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1324 - accuracy: 0.9614\n",
            "Epoch 116: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1338 - accuracy: 0.9612 - val_loss: 0.0702 - val_accuracy: 0.9827\n",
            "Epoch 117/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.9523\n",
            "Epoch 117: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1438 - accuracy: 0.9522 - val_loss: 0.1019 - val_accuracy: 0.9539\n",
            "Epoch 118/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1400 - accuracy: 0.9540\n",
            "Epoch 118: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1400 - accuracy: 0.9540 - val_loss: 0.0849 - val_accuracy: 0.9827\n",
            "Epoch 119/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9532\n",
            "Epoch 119: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1430 - accuracy: 0.9536 - val_loss: 0.0768 - val_accuracy: 0.9798\n",
            "Epoch 120/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1472 - accuracy: 0.9549\n",
            "Epoch 120: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1469 - accuracy: 0.9551 - val_loss: 0.0754 - val_accuracy: 0.9798\n",
            "Epoch 121/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1319 - accuracy: 0.9582\n",
            "Epoch 121: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1316 - accuracy: 0.9583 - val_loss: 0.1753 - val_accuracy: 0.9222\n",
            "Epoch 122/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9497\n",
            "Epoch 122: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1408 - accuracy: 0.9500 - val_loss: 0.1090 - val_accuracy: 0.9510\n",
            "Epoch 123/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1480 - accuracy: 0.9561\n",
            "Epoch 123: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1474 - accuracy: 0.9558 - val_loss: 0.1046 - val_accuracy: 0.9539\n",
            "Epoch 124/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1386 - accuracy: 0.9565\n",
            "Epoch 124: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1391 - accuracy: 0.9561 - val_loss: 0.1577 - val_accuracy: 0.9337\n",
            "Epoch 125/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1339 - accuracy: 0.9596\n",
            "Epoch 125: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1359 - accuracy: 0.9586 - val_loss: 0.0841 - val_accuracy: 0.9827\n",
            "Epoch 126/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1360 - accuracy: 0.9601\n",
            "Epoch 126: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1360 - accuracy: 0.9601 - val_loss: 0.1046 - val_accuracy: 0.9539\n",
            "Epoch 127/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1300 - accuracy: 0.9586\n",
            "Epoch 127: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1300 - accuracy: 0.9586 - val_loss: 0.1241 - val_accuracy: 0.9395\n",
            "Epoch 128/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1447 - accuracy: 0.9488\n",
            "Epoch 128: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1445 - accuracy: 0.9489 - val_loss: 0.0856 - val_accuracy: 0.9683\n",
            "Epoch 129/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1413 - accuracy: 0.9556\n",
            "Epoch 129: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1421 - accuracy: 0.9551 - val_loss: 0.0822 - val_accuracy: 0.9597\n",
            "Epoch 130/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9592\n",
            "Epoch 130: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1289 - accuracy: 0.9594 - val_loss: 0.0836 - val_accuracy: 0.9741\n",
            "Epoch 131/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9594\n",
            "Epoch 131: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1385 - accuracy: 0.9594 - val_loss: 0.2234 - val_accuracy: 0.8559\n",
            "Epoch 132/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1294 - accuracy: 0.9558\n",
            "Epoch 132: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1294 - accuracy: 0.9558 - val_loss: 0.0944 - val_accuracy: 0.9741\n",
            "Epoch 133/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9584\n",
            "Epoch 133: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1352 - accuracy: 0.9583 - val_loss: 0.1555 - val_accuracy: 0.9452\n",
            "Epoch 134/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9592\n",
            "Epoch 134: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 0.1308 - accuracy: 0.9590 - val_loss: 0.1056 - val_accuracy: 0.9712\n",
            "Epoch 135/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1354 - accuracy: 0.9556\n",
            "Epoch 135: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 6s 17ms/step - loss: 0.1352 - accuracy: 0.9558 - val_loss: 0.0675 - val_accuracy: 0.9683\n",
            "Epoch 136/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1312 - accuracy: 0.9566\n",
            "Epoch 136: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1320 - accuracy: 0.9565 - val_loss: 0.0818 - val_accuracy: 0.9654\n",
            "Epoch 137/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9649\n",
            "Epoch 137: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1168 - accuracy: 0.9648 - val_loss: 0.0796 - val_accuracy: 0.9827\n",
            "Epoch 138/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9589\n",
            "Epoch 138: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1315 - accuracy: 0.9586 - val_loss: 0.0745 - val_accuracy: 0.9741\n",
            "Epoch 139/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9613\n",
            "Epoch 139: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1263 - accuracy: 0.9615 - val_loss: 0.1032 - val_accuracy: 0.9827\n",
            "Epoch 140/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1394 - accuracy: 0.9562\n",
            "Epoch 140: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1398 - accuracy: 0.9558 - val_loss: 0.0908 - val_accuracy: 0.9769\n",
            "Epoch 141/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9568\n",
            "Epoch 141: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1360 - accuracy: 0.9572 - val_loss: 0.0754 - val_accuracy: 0.9827\n",
            "Epoch 142/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9609\n",
            "Epoch 142: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1254 - accuracy: 0.9608 - val_loss: 0.1224 - val_accuracy: 0.9539\n",
            "Epoch 143/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1313 - accuracy: 0.9565\n",
            "Epoch 143: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1313 - accuracy: 0.9565 - val_loss: 0.0787 - val_accuracy: 0.9827\n",
            "Epoch 144/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9554\n",
            "Epoch 144: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1349 - accuracy: 0.9558 - val_loss: 0.1263 - val_accuracy: 0.9452\n",
            "Epoch 145/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1240 - accuracy: 0.9579\n",
            "Epoch 145: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1240 - accuracy: 0.9579 - val_loss: 0.1191 - val_accuracy: 0.9654\n",
            "Epoch 146/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9550\n",
            "Epoch 146: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1345 - accuracy: 0.9554 - val_loss: 0.0964 - val_accuracy: 0.9597\n",
            "Epoch 147/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1278 - accuracy: 0.9597\n",
            "Epoch 147: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1273 - accuracy: 0.9597 - val_loss: 0.0720 - val_accuracy: 0.9712\n",
            "Epoch 148/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1393 - accuracy: 0.9538\n",
            "Epoch 148: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1396 - accuracy: 0.9536 - val_loss: 0.0677 - val_accuracy: 0.9769\n",
            "Epoch 149/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1349 - accuracy: 0.9592\n",
            "Epoch 149: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1350 - accuracy: 0.9590 - val_loss: 0.1272 - val_accuracy: 0.9424\n",
            "Epoch 150/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9612\n",
            "Epoch 150: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1247 - accuracy: 0.9612 - val_loss: 0.0800 - val_accuracy: 0.9625\n",
            "Epoch 151/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9569\n",
            "Epoch 151: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1424 - accuracy: 0.9569 - val_loss: 0.0612 - val_accuracy: 0.9798\n",
            "Epoch 152/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1286 - accuracy: 0.9548\n",
            "Epoch 152: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1284 - accuracy: 0.9551 - val_loss: 0.0829 - val_accuracy: 0.9683\n",
            "Epoch 153/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9631\n",
            "Epoch 153: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1239 - accuracy: 0.9630 - val_loss: 0.0888 - val_accuracy: 0.9568\n",
            "Epoch 154/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9603\n",
            "Epoch 154: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1249 - accuracy: 0.9597 - val_loss: 0.0574 - val_accuracy: 0.9827\n",
            "Epoch 155/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1196 - accuracy: 0.9589\n",
            "Epoch 155: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1191 - accuracy: 0.9594 - val_loss: 0.0643 - val_accuracy: 0.9769\n",
            "Epoch 156/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1290 - accuracy: 0.9623\n",
            "Epoch 156: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1286 - accuracy: 0.9622 - val_loss: 0.0695 - val_accuracy: 0.9741\n",
            "Epoch 157/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9644\n",
            "Epoch 157: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1193 - accuracy: 0.9644 - val_loss: 0.1110 - val_accuracy: 0.9481\n",
            "Epoch 158/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.9579\n",
            "Epoch 158: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1332 - accuracy: 0.9579 - val_loss: 0.1272 - val_accuracy: 0.9366\n",
            "Epoch 159/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1309 - accuracy: 0.9612\n",
            "Epoch 159: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1313 - accuracy: 0.9612 - val_loss: 0.0848 - val_accuracy: 0.9741\n",
            "Epoch 160/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1298 - accuracy: 0.9527\n",
            "Epoch 160: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1288 - accuracy: 0.9529 - val_loss: 0.0811 - val_accuracy: 0.9798\n",
            "Epoch 161/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9596\n",
            "Epoch 161: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1340 - accuracy: 0.9601 - val_loss: 0.0975 - val_accuracy: 0.9597\n",
            "Epoch 162/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9560\n",
            "Epoch 162: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1300 - accuracy: 0.9554 - val_loss: 0.1197 - val_accuracy: 0.9539\n",
            "Epoch 163/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1344 - accuracy: 0.9559\n",
            "Epoch 163: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1368 - accuracy: 0.9554 - val_loss: 0.0859 - val_accuracy: 0.9625\n",
            "Epoch 164/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9600\n",
            "Epoch 164: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1302 - accuracy: 0.9601 - val_loss: 0.0862 - val_accuracy: 0.9769\n",
            "Epoch 165/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1304 - accuracy: 0.9589\n",
            "Epoch 165: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1300 - accuracy: 0.9590 - val_loss: 0.0741 - val_accuracy: 0.9827\n",
            "Epoch 166/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9592\n",
            "Epoch 166: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1184 - accuracy: 0.9594 - val_loss: 0.0852 - val_accuracy: 0.9654\n",
            "Epoch 167/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9596\n",
            "Epoch 167: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1198 - accuracy: 0.9594 - val_loss: 0.1294 - val_accuracy: 0.9452\n",
            "Epoch 168/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9614\n",
            "Epoch 168: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1270 - accuracy: 0.9615 - val_loss: 0.0784 - val_accuracy: 0.9712\n",
            "Epoch 169/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1208 - accuracy: 0.9580\n",
            "Epoch 169: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1206 - accuracy: 0.9579 - val_loss: 0.0727 - val_accuracy: 0.9712\n",
            "Epoch 170/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9630\n",
            "Epoch 170: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1241 - accuracy: 0.9630 - val_loss: 0.0641 - val_accuracy: 0.9741\n",
            "Epoch 171/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9619\n",
            "Epoch 171: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1185 - accuracy: 0.9619 - val_loss: 0.0744 - val_accuracy: 0.9712\n",
            "Epoch 172/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9608\n",
            "Epoch 172: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1177 - accuracy: 0.9608 - val_loss: 0.0845 - val_accuracy: 0.9712\n",
            "Epoch 173/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1154 - accuracy: 0.9644\n",
            "Epoch 173: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1154 - accuracy: 0.9644 - val_loss: 0.0718 - val_accuracy: 0.9597\n",
            "Epoch 174/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9570\n",
            "Epoch 174: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1287 - accuracy: 0.9569 - val_loss: 0.0864 - val_accuracy: 0.9712\n",
            "Epoch 175/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9633\n",
            "Epoch 175: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1165 - accuracy: 0.9633 - val_loss: 0.0657 - val_accuracy: 0.9827\n",
            "Epoch 176/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9561\n",
            "Epoch 176: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1219 - accuracy: 0.9558 - val_loss: 0.0794 - val_accuracy: 0.9798\n",
            "Epoch 177/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1263 - accuracy: 0.9607\n",
            "Epoch 177: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1259 - accuracy: 0.9608 - val_loss: 0.0931 - val_accuracy: 0.9712\n",
            "Epoch 178/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1221 - accuracy: 0.9562\n",
            "Epoch 178: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1219 - accuracy: 0.9565 - val_loss: 0.0706 - val_accuracy: 0.9769\n",
            "Epoch 179/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9594\n",
            "Epoch 179: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1169 - accuracy: 0.9594 - val_loss: 0.0521 - val_accuracy: 0.9827\n",
            "Epoch 180/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9637\n",
            "Epoch 180: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1183 - accuracy: 0.9633 - val_loss: 0.0729 - val_accuracy: 0.9856\n",
            "Epoch 181/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9618\n",
            "Epoch 181: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1275 - accuracy: 0.9619 - val_loss: 0.1348 - val_accuracy: 0.9395\n",
            "Epoch 182/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9580\n",
            "Epoch 182: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1245 - accuracy: 0.9583 - val_loss: 0.0662 - val_accuracy: 0.9798\n",
            "Epoch 183/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9644\n",
            "Epoch 183: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1141 - accuracy: 0.9644 - val_loss: 0.0655 - val_accuracy: 0.9712\n",
            "Epoch 184/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9652\n",
            "Epoch 184: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1087 - accuracy: 0.9655 - val_loss: 0.0568 - val_accuracy: 0.9798\n",
            "Epoch 185/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9588\n",
            "Epoch 185: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1251 - accuracy: 0.9586 - val_loss: 0.0741 - val_accuracy: 0.9683\n",
            "Epoch 186/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9652\n",
            "Epoch 186: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1132 - accuracy: 0.9651 - val_loss: 0.0832 - val_accuracy: 0.9539\n",
            "Epoch 187/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9597\n",
            "Epoch 187: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1271 - accuracy: 0.9597 - val_loss: 0.0664 - val_accuracy: 0.9769\n",
            "Epoch 188/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9663\n",
            "Epoch 188: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1110 - accuracy: 0.9662 - val_loss: 0.0613 - val_accuracy: 0.9827\n",
            "Epoch 189/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1166 - accuracy: 0.9601\n",
            "Epoch 189: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1166 - accuracy: 0.9601 - val_loss: 0.0824 - val_accuracy: 0.9683\n",
            "Epoch 190/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9649\n",
            "Epoch 190: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1106 - accuracy: 0.9648 - val_loss: 0.0691 - val_accuracy: 0.9798\n",
            "Epoch 191/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9640\n",
            "Epoch 191: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1171 - accuracy: 0.9644 - val_loss: 0.0918 - val_accuracy: 0.9625\n",
            "Epoch 192/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9602\n",
            "Epoch 192: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1271 - accuracy: 0.9604 - val_loss: 0.0718 - val_accuracy: 0.9712\n",
            "Epoch 193/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9597\n",
            "Epoch 193: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1199 - accuracy: 0.9601 - val_loss: 0.0611 - val_accuracy: 0.9769\n",
            "Epoch 194/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9552\n",
            "Epoch 194: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1286 - accuracy: 0.9554 - val_loss: 0.0709 - val_accuracy: 0.9798\n",
            "Epoch 195/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1092 - accuracy: 0.9687\n",
            "Epoch 195: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1092 - accuracy: 0.9687 - val_loss: 0.0609 - val_accuracy: 0.9769\n",
            "Epoch 196/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9554\n",
            "Epoch 196: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1250 - accuracy: 0.9551 - val_loss: 0.0711 - val_accuracy: 0.9769\n",
            "Epoch 197/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9607\n",
            "Epoch 197: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1165 - accuracy: 0.9608 - val_loss: 0.0739 - val_accuracy: 0.9798\n",
            "Epoch 198/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9591\n",
            "Epoch 198: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1212 - accuracy: 0.9590 - val_loss: 0.0618 - val_accuracy: 0.9798\n",
            "Epoch 199/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9668\n",
            "Epoch 199: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1152 - accuracy: 0.9669 - val_loss: 0.0689 - val_accuracy: 0.9798\n",
            "Epoch 200/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1147 - accuracy: 0.9617\n",
            "Epoch 200: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1146 - accuracy: 0.9619 - val_loss: 0.0820 - val_accuracy: 0.9798\n",
            "Epoch 201/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9629\n",
            "Epoch 201: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1100 - accuracy: 0.9633 - val_loss: 0.0673 - val_accuracy: 0.9712\n",
            "Epoch 202/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9620\n",
            "Epoch 202: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1118 - accuracy: 0.9619 - val_loss: 0.0585 - val_accuracy: 0.9798\n",
            "Epoch 203/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9643\n",
            "Epoch 203: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1073 - accuracy: 0.9644 - val_loss: 0.0614 - val_accuracy: 0.9856\n",
            "Epoch 204/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9612\n",
            "Epoch 204: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1115 - accuracy: 0.9612 - val_loss: 0.0655 - val_accuracy: 0.9798\n",
            "Epoch 205/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1164 - accuracy: 0.9605\n",
            "Epoch 205: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1156 - accuracy: 0.9608 - val_loss: 0.0615 - val_accuracy: 0.9798\n",
            "Epoch 206/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9597\n",
            "Epoch 206: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1155 - accuracy: 0.9601 - val_loss: 0.0902 - val_accuracy: 0.9597\n",
            "Epoch 207/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1110 - accuracy: 0.9611\n",
            "Epoch 207: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1100 - accuracy: 0.9615 - val_loss: 0.1343 - val_accuracy: 0.9452\n",
            "Epoch 208/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9584\n",
            "Epoch 208: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1189 - accuracy: 0.9583 - val_loss: 0.0754 - val_accuracy: 0.9827\n",
            "Epoch 209/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9641\n",
            "Epoch 209: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1125 - accuracy: 0.9644 - val_loss: 0.0646 - val_accuracy: 0.9827\n",
            "Epoch 210/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1143 - accuracy: 0.9633\n",
            "Epoch 210: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1134 - accuracy: 0.9637 - val_loss: 0.0697 - val_accuracy: 0.9683\n",
            "Epoch 211/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1094 - accuracy: 0.9651\n",
            "Epoch 211: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 13ms/step - loss: 0.1094 - accuracy: 0.9651 - val_loss: 0.0572 - val_accuracy: 0.9798\n",
            "Epoch 212/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9633\n",
            "Epoch 212: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1090 - accuracy: 0.9633 - val_loss: 0.0625 - val_accuracy: 0.9827\n",
            "Epoch 213/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1171 - accuracy: 0.9596\n",
            "Epoch 213: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1168 - accuracy: 0.9601 - val_loss: 0.0837 - val_accuracy: 0.9539\n",
            "Epoch 214/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9679\n",
            "Epoch 214: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1024 - accuracy: 0.9680 - val_loss: 0.0573 - val_accuracy: 0.9769\n",
            "Epoch 215/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1111 - accuracy: 0.9631\n",
            "Epoch 215: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1114 - accuracy: 0.9630 - val_loss: 0.0832 - val_accuracy: 0.9827\n",
            "Epoch 216/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1162 - accuracy: 0.9592\n",
            "Epoch 216: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1163 - accuracy: 0.9594 - val_loss: 0.0916 - val_accuracy: 0.9683\n",
            "Epoch 217/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9638\n",
            "Epoch 217: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1148 - accuracy: 0.9640 - val_loss: 0.0630 - val_accuracy: 0.9769\n",
            "Epoch 218/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1105 - accuracy: 0.9603\n",
            "Epoch 218: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1105 - accuracy: 0.9604 - val_loss: 0.0560 - val_accuracy: 0.9798\n",
            "Epoch 219/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9601\n",
            "Epoch 219: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1184 - accuracy: 0.9601 - val_loss: 0.0623 - val_accuracy: 0.9769\n",
            "Epoch 220/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1214 - accuracy: 0.9574\n",
            "Epoch 220: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1214 - accuracy: 0.9576 - val_loss: 0.0557 - val_accuracy: 0.9856\n",
            "Epoch 221/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9664\n",
            "Epoch 221: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1089 - accuracy: 0.9666 - val_loss: 0.0589 - val_accuracy: 0.9856\n",
            "Epoch 222/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9597\n",
            "Epoch 222: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1220 - accuracy: 0.9601 - val_loss: 0.0783 - val_accuracy: 0.9683\n",
            "Epoch 223/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1035 - accuracy: 0.9617\n",
            "Epoch 223: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1038 - accuracy: 0.9615 - val_loss: 0.0552 - val_accuracy: 0.9798\n",
            "Epoch 224/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9631\n",
            "Epoch 224: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1138 - accuracy: 0.9630 - val_loss: 0.0660 - val_accuracy: 0.9827\n",
            "Epoch 225/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9654\n",
            "Epoch 225: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1042 - accuracy: 0.9655 - val_loss: 0.0559 - val_accuracy: 0.9798\n",
            "Epoch 226/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9655\n",
            "Epoch 226: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1116 - accuracy: 0.9655 - val_loss: 0.0706 - val_accuracy: 0.9712\n",
            "Epoch 227/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1102 - accuracy: 0.9652\n",
            "Epoch 227: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1107 - accuracy: 0.9648 - val_loss: 0.0978 - val_accuracy: 0.9712\n",
            "Epoch 228/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9669\n",
            "Epoch 228: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1046 - accuracy: 0.9669 - val_loss: 0.0684 - val_accuracy: 0.9741\n",
            "Epoch 229/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9669\n",
            "Epoch 229: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0993 - accuracy: 0.9669 - val_loss: 0.0626 - val_accuracy: 0.9712\n",
            "Epoch 230/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9622\n",
            "Epoch 230: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1076 - accuracy: 0.9615 - val_loss: 0.0816 - val_accuracy: 0.9827\n",
            "Epoch 231/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9645\n",
            "Epoch 231: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1069 - accuracy: 0.9648 - val_loss: 0.0768 - val_accuracy: 0.9827\n",
            "Epoch 232/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9622\n",
            "Epoch 232: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 6s 17ms/step - loss: 0.1203 - accuracy: 0.9622 - val_loss: 0.0779 - val_accuracy: 0.9597\n",
            "Epoch 233/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1021 - accuracy: 0.9645\n",
            "Epoch 233: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1021 - accuracy: 0.9648 - val_loss: 0.0693 - val_accuracy: 0.9712\n",
            "Epoch 234/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1119 - accuracy: 0.9584\n",
            "Epoch 234: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1120 - accuracy: 0.9583 - val_loss: 0.0552 - val_accuracy: 0.9856\n",
            "Epoch 235/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9679\n",
            "Epoch 235: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1015 - accuracy: 0.9676 - val_loss: 0.0798 - val_accuracy: 0.9741\n",
            "Epoch 236/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9625\n",
            "Epoch 236: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1174 - accuracy: 0.9622 - val_loss: 0.0719 - val_accuracy: 0.9741\n",
            "Epoch 237/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1142 - accuracy: 0.9664\n",
            "Epoch 237: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1141 - accuracy: 0.9666 - val_loss: 0.1867 - val_accuracy: 0.9135\n",
            "Epoch 238/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9623\n",
            "Epoch 238: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1087 - accuracy: 0.9626 - val_loss: 0.0705 - val_accuracy: 0.9827\n",
            "Epoch 239/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1047 - accuracy: 0.9668\n",
            "Epoch 239: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1056 - accuracy: 0.9666 - val_loss: 0.0905 - val_accuracy: 0.9597\n",
            "Epoch 240/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1073 - accuracy: 0.9597\n",
            "Epoch 240: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1073 - accuracy: 0.9597 - val_loss: 0.0805 - val_accuracy: 0.9712\n",
            "Epoch 241/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9600\n",
            "Epoch 241: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1074 - accuracy: 0.9597 - val_loss: 0.0592 - val_accuracy: 0.9769\n",
            "Epoch 242/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.0921 - accuracy: 0.9713\n",
            "Epoch 242: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0922 - accuracy: 0.9712 - val_loss: 0.0611 - val_accuracy: 0.9712\n",
            "Epoch 243/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.0998 - accuracy: 0.9723\n",
            "Epoch 243: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0998 - accuracy: 0.9723 - val_loss: 0.1041 - val_accuracy: 0.9597\n",
            "Epoch 244/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1072 - accuracy: 0.9656\n",
            "Epoch 244: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1069 - accuracy: 0.9658 - val_loss: 0.0630 - val_accuracy: 0.9798\n",
            "Epoch 245/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1082 - accuracy: 0.9629\n",
            "Epoch 245: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1079 - accuracy: 0.9630 - val_loss: 0.0614 - val_accuracy: 0.9769\n",
            "Epoch 246/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1086 - accuracy: 0.9603\n",
            "Epoch 246: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1081 - accuracy: 0.9608 - val_loss: 0.0495 - val_accuracy: 0.9798\n",
            "Epoch 247/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9594\n",
            "Epoch 247: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1093 - accuracy: 0.9597 - val_loss: 0.0769 - val_accuracy: 0.9712\n",
            "Epoch 248/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9612\n",
            "Epoch 248: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1181 - accuracy: 0.9608 - val_loss: 0.1132 - val_accuracy: 0.9510\n",
            "Epoch 249/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.0987 - accuracy: 0.9648\n",
            "Epoch 249: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0985 - accuracy: 0.9651 - val_loss: 0.0676 - val_accuracy: 0.9769\n",
            "Epoch 250/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9679\n",
            "Epoch 250: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1037 - accuracy: 0.9676 - val_loss: 0.0659 - val_accuracy: 0.9769\n",
            "Epoch 251/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.0984 - accuracy: 0.9661\n",
            "Epoch 251: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0982 - accuracy: 0.9662 - val_loss: 0.0603 - val_accuracy: 0.9769\n",
            "Epoch 252/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9612\n",
            "Epoch 252: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1103 - accuracy: 0.9612 - val_loss: 0.0570 - val_accuracy: 0.9769\n",
            "Epoch 253/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9617\n",
            "Epoch 253: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1075 - accuracy: 0.9615 - val_loss: 0.0621 - val_accuracy: 0.9741\n",
            "Epoch 254/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9592\n",
            "Epoch 254: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1131 - accuracy: 0.9594 - val_loss: 0.0739 - val_accuracy: 0.9741\n",
            "Epoch 255/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9603\n",
            "Epoch 255: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1066 - accuracy: 0.9604 - val_loss: 0.0708 - val_accuracy: 0.9798\n",
            "Epoch 256/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9602\n",
            "Epoch 256: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1088 - accuracy: 0.9604 - val_loss: 0.0499 - val_accuracy: 0.9856\n",
            "Epoch 257/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9658\n",
            "Epoch 257: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1036 - accuracy: 0.9658 - val_loss: 0.0504 - val_accuracy: 0.9769\n",
            "Epoch 258/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1041 - accuracy: 0.9679\n",
            "Epoch 258: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1049 - accuracy: 0.9676 - val_loss: 0.0793 - val_accuracy: 0.9712\n",
            "Epoch 259/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1016 - accuracy: 0.9646\n",
            "Epoch 259: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1014 - accuracy: 0.9648 - val_loss: 0.0593 - val_accuracy: 0.9827\n",
            "Epoch 260/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1144 - accuracy: 0.9586\n",
            "Epoch 260: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1144 - accuracy: 0.9586 - val_loss: 0.0562 - val_accuracy: 0.9741\n",
            "Epoch 261/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1081 - accuracy: 0.9674\n",
            "Epoch 261: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1076 - accuracy: 0.9676 - val_loss: 0.0658 - val_accuracy: 0.9712\n",
            "Epoch 262/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.0989 - accuracy: 0.9695\n",
            "Epoch 262: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0984 - accuracy: 0.9698 - val_loss: 0.0618 - val_accuracy: 0.9712\n",
            "Epoch 263/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1013 - accuracy: 0.9636\n",
            "Epoch 263: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1014 - accuracy: 0.9633 - val_loss: 0.0620 - val_accuracy: 0.9798\n",
            "Epoch 264/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1010 - accuracy: 0.9655\n",
            "Epoch 264: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1010 - accuracy: 0.9655 - val_loss: 0.0714 - val_accuracy: 0.9712\n",
            "Epoch 265/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1075 - accuracy: 0.9603\n",
            "Epoch 265: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1075 - accuracy: 0.9601 - val_loss: 0.0621 - val_accuracy: 0.9798\n",
            "Epoch 266/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1021 - accuracy: 0.9684\n",
            "Epoch 266: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1015 - accuracy: 0.9687 - val_loss: 0.0673 - val_accuracy: 0.9712\n",
            "Epoch 267/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9654\n",
            "Epoch 267: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0994 - accuracy: 0.9655 - val_loss: 0.0628 - val_accuracy: 0.9798\n",
            "Epoch 268/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1112 - accuracy: 0.9609\n",
            "Epoch 268: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1109 - accuracy: 0.9612 - val_loss: 0.0506 - val_accuracy: 0.9798\n",
            "Epoch 269/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1083 - accuracy: 0.9635\n",
            "Epoch 269: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1082 - accuracy: 0.9633 - val_loss: 0.0716 - val_accuracy: 0.9597\n",
            "Epoch 270/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.0958 - accuracy: 0.9648\n",
            "Epoch 270: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0955 - accuracy: 0.9651 - val_loss: 0.0583 - val_accuracy: 0.9798\n",
            "Epoch 271/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9646\n",
            "Epoch 271: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1039 - accuracy: 0.9648 - val_loss: 0.0670 - val_accuracy: 0.9712\n",
            "Epoch 272/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.0995 - accuracy: 0.9648\n",
            "Epoch 272: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0994 - accuracy: 0.9648 - val_loss: 0.0656 - val_accuracy: 0.9741\n",
            "Epoch 273/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9673\n",
            "Epoch 273: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1117 - accuracy: 0.9673 - val_loss: 0.0455 - val_accuracy: 0.9856\n",
            "Epoch 274/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1068 - accuracy: 0.9646\n",
            "Epoch 274: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1065 - accuracy: 0.9648 - val_loss: 0.0678 - val_accuracy: 0.9712\n",
            "Epoch 275/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9605\n",
            "Epoch 275: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1104 - accuracy: 0.9608 - val_loss: 0.0663 - val_accuracy: 0.9741\n",
            "Epoch 276/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9572\n",
            "Epoch 276: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1208 - accuracy: 0.9569 - val_loss: 0.0682 - val_accuracy: 0.9741\n",
            "Epoch 277/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.1009 - accuracy: 0.9653\n",
            "Epoch 277: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1005 - accuracy: 0.9655 - val_loss: 0.0623 - val_accuracy: 0.9827\n",
            "Epoch 278/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1028 - accuracy: 0.9677\n",
            "Epoch 278: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1026 - accuracy: 0.9680 - val_loss: 0.0512 - val_accuracy: 0.9827\n",
            "Epoch 279/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.0988 - accuracy: 0.9629\n",
            "Epoch 279: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0991 - accuracy: 0.9630 - val_loss: 0.0620 - val_accuracy: 0.9769\n",
            "Epoch 280/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9658\n",
            "Epoch 280: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1032 - accuracy: 0.9658 - val_loss: 0.0551 - val_accuracy: 0.9827\n",
            "Epoch 281/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1000 - accuracy: 0.9637\n",
            "Epoch 281: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1000 - accuracy: 0.9637 - val_loss: 0.0946 - val_accuracy: 0.9597\n",
            "Epoch 282/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9655\n",
            "Epoch 282: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1094 - accuracy: 0.9651 - val_loss: 0.0627 - val_accuracy: 0.9741\n",
            "Epoch 283/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1040 - accuracy: 0.9654\n",
            "Epoch 283: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1036 - accuracy: 0.9655 - val_loss: 0.0591 - val_accuracy: 0.9798\n",
            "Epoch 284/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1069 - accuracy: 0.9674\n",
            "Epoch 284: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1068 - accuracy: 0.9673 - val_loss: 0.0804 - val_accuracy: 0.9683\n",
            "Epoch 285/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.0986 - accuracy: 0.9688\n",
            "Epoch 285: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0994 - accuracy: 0.9687 - val_loss: 0.0816 - val_accuracy: 0.9827\n",
            "Epoch 286/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1090 - accuracy: 0.9673\n",
            "Epoch 286: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1090 - accuracy: 0.9673 - val_loss: 0.0650 - val_accuracy: 0.9683\n",
            "Epoch 287/300\n",
            "345/348 [============================>.] - ETA: 0s - loss: 0.1026 - accuracy: 0.9670\n",
            "Epoch 287: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1019 - accuracy: 0.9673 - val_loss: 0.0635 - val_accuracy: 0.9654\n",
            "Epoch 288/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1091 - accuracy: 0.9657\n",
            "Epoch 288: val_accuracy did not improve from 0.98559\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1088 - accuracy: 0.9658 - val_loss: 0.0678 - val_accuracy: 0.9654\n",
            "Epoch 289/300\n",
            "344/348 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9644\n",
            "Epoch 289: val_accuracy improved from 0.98559 to 0.98847, saving model to weights/07-07-2022_05:42:07_weights.289.h5\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1095 - accuracy: 0.9648 - val_loss: 0.0435 - val_accuracy: 0.9885\n",
            "Epoch 290/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9636\n",
            "Epoch 290: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1015 - accuracy: 0.9633 - val_loss: 0.0666 - val_accuracy: 0.9683\n",
            "Epoch 291/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1108 - accuracy: 0.9607\n",
            "Epoch 291: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1106 - accuracy: 0.9608 - val_loss: 0.0561 - val_accuracy: 0.9741\n",
            "Epoch 292/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.0916 - accuracy: 0.9671\n",
            "Epoch 292: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0911 - accuracy: 0.9673 - val_loss: 0.0494 - val_accuracy: 0.9856\n",
            "Epoch 293/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.0927 - accuracy: 0.9701\n",
            "Epoch 293: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0925 - accuracy: 0.9702 - val_loss: 0.0591 - val_accuracy: 0.9769\n",
            "Epoch 294/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9662\n",
            "Epoch 294: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0918 - accuracy: 0.9662 - val_loss: 0.0892 - val_accuracy: 0.9597\n",
            "Epoch 295/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1104 - accuracy: 0.9597\n",
            "Epoch 295: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 15ms/step - loss: 0.1104 - accuracy: 0.9597 - val_loss: 0.0497 - val_accuracy: 0.9856\n",
            "Epoch 296/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.1012 - accuracy: 0.9633\n",
            "Epoch 296: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1012 - accuracy: 0.9633 - val_loss: 0.0475 - val_accuracy: 0.9769\n",
            "Epoch 297/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.0961 - accuracy: 0.9685\n",
            "Epoch 297: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0963 - accuracy: 0.9684 - val_loss: 0.0635 - val_accuracy: 0.9683\n",
            "Epoch 298/300\n",
            "346/348 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9671\n",
            "Epoch 298: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0977 - accuracy: 0.9673 - val_loss: 0.0495 - val_accuracy: 0.9856\n",
            "Epoch 299/300\n",
            "348/348 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9669\n",
            "Epoch 299: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.0985 - accuracy: 0.9669 - val_loss: 0.0702 - val_accuracy: 0.9827\n",
            "Epoch 300/300\n",
            "347/348 [============================>.] - ETA: 0s - loss: 0.1006 - accuracy: 0.9661\n",
            "Epoch 300: val_accuracy did not improve from 0.98847\n",
            "348/348 [==============================] - 5s 14ms/step - loss: 0.1006 - accuracy: 0.9662 - val_loss: 0.0502 - val_accuracy: 0.9769\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "                    traingen,\n",
        "                    epochs=300,\n",
        "                    callbacks=[monitor_progress],\n",
        "                    validation_data=validationgen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZC5AvrwnSZH8",
        "outputId": "f082d6a9-0c07-409f-e98d-71673e490be8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.028958192095160484, 0.9885386824607849]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate_generator(testgen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JxQSMzU_SZXf",
        "outputId": "7b2dcdb1-0640-4f5b-f229-64f389b9e784"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(\"acc_31_05_19.png\")\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.savefig(\"loss_31_05_19.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zr6aFsrFMiCS"
      },
      "source": [
        "**Eval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYYI23qJMnyx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import glob\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFj_gcTsDWyU"
      },
      "outputs": [],
      "source": [
        "test_model = load_model(\"/content/gdrive/MyDrive/my_models/weights/07-07-2022_05:42:07_weights.35.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t76KUajWaVXt",
        "outputId": "78b34b0a-5dbb-4ec1-eb02-d75e060a01e4"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/MyDrive/25by25.zip\n",
            "replace /content/gdrive/MyDrive/25by25/test/negative/image1.neg196.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "#!unzip /content/gdrive/MyDrive/25by25.zip -d /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC1fy2baMytf"
      },
      "outputs": [],
      "source": [
        "root_img_path =\"/content/gdrive/MyDrive/25by25/test\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vMxJWBQNBPe"
      },
      "outputs": [],
      "source": [
        "def load_images(img_path):\n",
        "    return np.array([cv2.imread(img_path+file) for file in os.listdir(img_path)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smuBR1jbaX_4"
      },
      "outputs": [],
      "source": [
        "def get_label(prediction,idx):\n",
        "    if prediction[idx]<0.5:\n",
        "        return \"Negative\"\n",
        "    return \"Positive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPB_2Oz0aaeS"
      },
      "outputs": [],
      "source": [
        "def get_neg_class(prediction_class, N):\n",
        "    arr = []\n",
        "    while len(arr)<N+1:\n",
        "        idx = random.randint(0,len(prediction_class)-1)\n",
        "        if prediction_class[idx]<0.5 and idx not in arr:\n",
        "            arr.append(idx)\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "helaiVBuac3D"
      },
      "outputs": [],
      "source": [
        "def get_pos_class(prediction_class, N):\n",
        "    arr = []\n",
        "    while len(arr)<N+1:\n",
        "        idx = random.randint(0,len(prediction_class)-1)\n",
        "        if prediction_class[idx]>=0.5 and idx not in arr:\n",
        "            arr.append(idx)\n",
        "    return arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nMiIp-saflz"
      },
      "outputs": [],
      "source": [
        "# Create positive and negative batch for predictions\n",
        "positive_class = load_images(os.path.join(root_img_path,\"positive/\"))\n",
        "negative_class = load_images(os.path.join(root_img_path,\"negative/\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DgsmsaFaiCG",
        "outputId": "bb0df70c-5126-43d9-dd13-5fb22a6ec60e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "138"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Predictions for positive class\n",
        "#predicted_pos = model.predict_classes(positive_class)\n",
        "#predicted_pos = model.predict_on_batch(positive_class)\n",
        "predicted_pos = (test_model.predict(positive_class) >= 0.5).astype(\"int32\")\n",
        "len(predicted_pos)\n",
        "#print(predicted_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD1929YYak8S",
        "outputId": "05d531dc-97b3-4a48-f00f-1ad783c7ce70"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Predictions for negative class\n",
        "#predicted_neg = model.predict_on_batch(negative_class)\n",
        "#predicted_neg = model.predict_classes(negative_class)\n",
        "predicted_neg = (test_model.predict(negative_class) < 0.5).astype(\"int32\")\n",
        "len(predicted_neg)\n",
        "#print(predicted_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1izQ_WTanpj",
        "outputId": "01465312-65a8-4fe0-a15d-df24e6386d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive examples classified positive (TP):  128\n",
            "Positive examples classified negative (FN):  10\n"
          ]
        }
      ],
      "source": [
        "# Get TP(True Positives) and FN(False Negatives)\n",
        "TP = [p for p in predicted_pos if p >=0.5]\n",
        "FN = [p for p in predicted_pos if p<0.5]\n",
        "print(\"Positive examples classified positive (TP): \",len(TP))\n",
        "print(\"Positive examples classified negative (FN): \",len(FN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1H5eTjgaqdn",
        "outputId": "8eb252f0-b86c-43b4-f740-3661658d15d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative examples classified positive (FP):  102\n",
            "Negative examples classified negative (TN):  109\n"
          ]
        }
      ],
      "source": [
        "# Get TN(True Negatives) and FP(False Positives)\n",
        "FP = [p for p in predicted_neg if p >=0.5]\n",
        "TN = [p for p in predicted_neg if p<0.5]\n",
        "print(\"Negative examples classified positive (FP): \",len(FP))\n",
        "print(\"Negative examples classified negative (TN): \",len(TN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mzpThDjatDg"
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(tp, fp, tn, fn):\n",
        "    pr = tp/(tp+fp)\n",
        "    rc = tp/(tp+fn)\n",
        "    f1_score = 2*((pr*rc)/(pr+rc))\n",
        "    print(\"TP \\t FP \\t TN \\t FN\\n\")\n",
        "    print(str(tp)+\"\\t\"+str(fp)+\"\\t\"+str(tn)+\"\\t\"+str(fn))\n",
        "    return pr, rc, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BtFKJhXawIC",
        "outputId": "092fb53b-0473-46c7-ab14-bb7c9e1348c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TP \t FP \t TN \t FN\n",
            "\n",
            "128\t102\t109\t10\n",
            "\n",
            "\n",
            "Precision:\t    0.5565\n",
            "Recall:\t\t    0.9275\n",
            "F1_Score:\t    0.6957\n"
          ]
        }
      ],
      "source": [
        "# Confusion matrix\n",
        "Precision, Recall, F1_Score = get_confusion_matrix(len(TP),len(FP),len(TN),len(FN))\n",
        "print(\"\\n\")\n",
        "print(\"Precision:\\t\",f'{Precision:9.4f}')\n",
        "print(\"Recall:\\t\\t\",f'{Recall:9.4f}')\n",
        "print(\"F1_Score:\\t\",f'{F1_Score:9.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AUC-ROC.**"
      ],
      "metadata": {
        "id": "9uPwR7euabjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.metrics.AUC(\n",
        "    num_thresholds=200,\n",
        "    curve=\"ROC\",\n",
        "    summation_method=\"interpolation\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9r159mqbF2C",
        "outputId": "76271439-f0c7-4360-a38d-a719a76fc0ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.metrics.AUC at 0x7f93f653f750>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alM4Ra6qOhe6"
      },
      "source": [
        "**TEST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSjsS_dwQ5Ei"
      },
      "outputs": [],
      "source": [
        "!unzip /content/gdrive/MyDrive/whitefly_test_set-20220701T151630Z-001.zip  -d /content/gdrive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZo6-F6zTOnm"
      },
      "outputs": [],
      "source": [
        "my_tesdir = '/content/gdrive/MyDrive/whitefly'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO94WFuIQle4"
      },
      "outputs": [],
      "source": [
        "my_testgen = ImageDataGenerator(rescale=1/225.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "larKSDDISvMX",
        "outputId": "0c1727e9-8cb2-4365-b5ec-c171416f80a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 600 images belonging to 1 classes.\n"
          ]
        }
      ],
      "source": [
        "dataset = my_testgen.flow_from_directory(\n",
        "                               directory= my_tesdir,\n",
        "                                target_size=(25,25),\n",
        "                                batch_size = 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqOHhaXoORDi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HOeKuIPOYKT"
      },
      "outputs": [],
      "source": [
        "model = load_model(\"/content/gdrive/MyDrive/my_models/weights/07-07-2022_05:42:07_weights.35.h5\")\n",
        "probabilities = model.predict(dataset)\n",
        "#predictions = tf.argmax(probabilities, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgzywP20Od7K",
        "outputId": "c8f1ce59-d823-404e-f9ec-e1eb3154bd1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.77252465]\n",
            " [0.484906  ]\n",
            " [0.76001257]\n",
            " [0.9880741 ]\n",
            " [0.67949164]\n",
            " [0.58790356]\n",
            " [0.56243193]\n",
            " [0.792376  ]\n",
            " [0.9658201 ]\n",
            " [0.5658189 ]\n",
            " [0.62157136]\n",
            " [0.7275213 ]\n",
            " [0.32311654]\n",
            " [0.97768193]\n",
            " [0.49972597]\n",
            " [0.86916536]\n",
            " [0.74157906]\n",
            " [0.73101413]\n",
            " [0.6782288 ]\n",
            " [0.5800162 ]\n",
            " [0.5031597 ]\n",
            " [0.51532054]\n",
            " [0.4890126 ]\n",
            " [0.83163434]\n",
            " [0.6177912 ]\n",
            " [0.96566796]\n",
            " [0.6201483 ]\n",
            " [0.72305024]\n",
            " [0.5376467 ]\n",
            " [0.7150704 ]\n",
            " [0.7806207 ]\n",
            " [0.48855338]\n",
            " [0.4842787 ]\n",
            " [0.5446358 ]\n",
            " [0.52834654]\n",
            " [0.6774957 ]\n",
            " [0.69407505]\n",
            " [0.84445924]\n",
            " [0.49029356]\n",
            " [0.5675021 ]\n",
            " [0.62719434]\n",
            " [0.6783524 ]\n",
            " [0.66282177]\n",
            " [0.48363274]\n",
            " [0.48787475]\n",
            " [0.7190993 ]\n",
            " [0.7482949 ]\n",
            " [0.62652004]\n",
            " [0.8730228 ]\n",
            " [0.98733795]\n",
            " [0.6009612 ]\n",
            " [0.51054645]\n",
            " [0.6000543 ]\n",
            " [0.6326677 ]\n",
            " [0.9606927 ]\n",
            " [0.94728005]\n",
            " [0.5737541 ]\n",
            " [0.80028814]\n",
            " [0.87194836]\n",
            " [0.5345134 ]\n",
            " [0.9948873 ]\n",
            " [0.56179994]\n",
            " [0.83040774]\n",
            " [0.50266874]\n",
            " [0.55098695]\n",
            " [0.5312466 ]\n",
            " [0.86960685]\n",
            " [0.7669575 ]\n",
            " [0.46016815]\n",
            " [0.5771394 ]\n",
            " [0.57539326]\n",
            " [0.60050535]\n",
            " [0.46869135]\n",
            " [0.5791674 ]\n",
            " [0.49407566]\n",
            " [0.62837476]\n",
            " [0.9198178 ]\n",
            " [0.69835144]\n",
            " [0.47677347]\n",
            " [0.80911577]\n",
            " [0.51275325]\n",
            " [0.55340725]\n",
            " [0.6897572 ]\n",
            " [0.7166945 ]\n",
            " [0.45962477]\n",
            " [0.92654973]\n",
            " [0.5995682 ]\n",
            " [0.67454827]\n",
            " [0.57824576]\n",
            " [0.70481336]\n",
            " [0.7322772 ]\n",
            " [0.5186106 ]\n",
            " [0.97209   ]\n",
            " [0.47935817]\n",
            " [0.56227595]\n",
            " [0.48094836]\n",
            " [0.50403494]\n",
            " [0.5134629 ]\n",
            " [0.4885935 ]\n",
            " [0.7714135 ]\n",
            " [0.48855338]\n",
            " [0.90915227]\n",
            " [0.67149335]\n",
            " [0.5836793 ]\n",
            " [0.97915286]\n",
            " [0.6635784 ]\n",
            " [0.4676677 ]\n",
            " [0.7180094 ]\n",
            " [0.5308162 ]\n",
            " [0.46847787]\n",
            " [0.790695  ]\n",
            " [0.07292639]\n",
            " [0.4872133 ]\n",
            " [0.5093353 ]\n",
            " [0.48855338]\n",
            " [0.66259426]\n",
            " [0.51087075]\n",
            " [0.5504598 ]\n",
            " [0.5195721 ]\n",
            " [0.70271283]\n",
            " [0.6232842 ]\n",
            " [0.73080444]\n",
            " [0.5113264 ]\n",
            " [0.68814236]\n",
            " [0.9435812 ]\n",
            " [0.5336099 ]\n",
            " [0.45464   ]\n",
            " [0.5659174 ]\n",
            " [0.56963   ]\n",
            " [0.9593651 ]\n",
            " [0.5019576 ]\n",
            " [0.59359145]\n",
            " [0.61906886]\n",
            " [0.6645711 ]\n",
            " [0.5843364 ]\n",
            " [0.6010907 ]\n",
            " [0.55995405]\n",
            " [0.6194959 ]\n",
            " [0.5409627 ]\n",
            " [0.8286266 ]\n",
            " [0.5305866 ]\n",
            " [0.9356443 ]\n",
            " [0.6103745 ]\n",
            " [0.5509758 ]\n",
            " [0.5805294 ]\n",
            " [0.66607654]\n",
            " [0.6735473 ]\n",
            " [0.7086758 ]\n",
            " [0.55932736]\n",
            " [0.48877928]\n",
            " [0.5115983 ]\n",
            " [0.9041342 ]\n",
            " [0.5288553 ]\n",
            " [0.5713692 ]\n",
            " [0.68017304]\n",
            " [0.93324935]\n",
            " [0.84067565]\n",
            " [0.4803902 ]\n",
            " [0.71648   ]\n",
            " [0.48424825]\n",
            " [0.574388  ]\n",
            " [0.5458171 ]\n",
            " [0.5937035 ]\n",
            " [0.4602083 ]\n",
            " [0.47104856]\n",
            " [0.88540035]\n",
            " [0.9404443 ]\n",
            " [0.48855338]\n",
            " [0.89296085]\n",
            " [0.48600176]\n",
            " [0.85181934]\n",
            " [0.5866559 ]\n",
            " [0.7759013 ]\n",
            " [0.7381782 ]\n",
            " [0.48119363]\n",
            " [0.6881345 ]\n",
            " [0.6882362 ]\n",
            " [0.9715607 ]\n",
            " [0.4866378 ]\n",
            " [0.76083076]\n",
            " [0.44542524]\n",
            " [0.4647345 ]\n",
            " [0.45813483]\n",
            " [0.7461964 ]\n",
            " [0.47908637]\n",
            " [0.70771676]\n",
            " [0.4643233 ]\n",
            " [0.4545427 ]\n",
            " [0.6199307 ]\n",
            " [0.49469474]\n",
            " [0.86498153]\n",
            " [0.45972526]\n",
            " [0.82582045]\n",
            " [0.48855338]\n",
            " [0.5755149 ]\n",
            " [0.8284734 ]\n",
            " [0.93529737]\n",
            " [0.48855338]\n",
            " [0.92761254]\n",
            " [0.8229291 ]\n",
            " [0.5552283 ]\n",
            " [0.66295886]\n",
            " [0.5672348 ]\n",
            " [0.53405404]\n",
            " [0.80143857]\n",
            " [0.7175786 ]\n",
            " [0.93318045]\n",
            " [0.95361036]\n",
            " [0.6400842 ]\n",
            " [0.84828603]\n",
            " [0.7992183 ]\n",
            " [0.66890997]\n",
            " [0.5548946 ]\n",
            " [0.6209365 ]\n",
            " [0.47034895]\n",
            " [0.72137314]\n",
            " [0.47870335]\n",
            " [0.4810531 ]\n",
            " [0.4837857 ]\n",
            " [0.83560145]\n",
            " [0.5547722 ]\n",
            " [0.48008984]\n",
            " [0.6682251 ]\n",
            " [0.7005477 ]\n",
            " [0.48855338]\n",
            " [0.4746423 ]\n",
            " [0.55465966]\n",
            " [0.62639064]\n",
            " [0.5127693 ]\n",
            " [0.4668316 ]\n",
            " [0.70432246]\n",
            " [0.5896441 ]\n",
            " [0.57202566]\n",
            " [0.68289703]\n",
            " [0.55781704]\n",
            " [0.9520532 ]\n",
            " [0.4801076 ]\n",
            " [0.5391745 ]\n",
            " [0.7127839 ]\n",
            " [0.5151036 ]\n",
            " [0.6138457 ]\n",
            " [0.8597261 ]\n",
            " [0.5692775 ]\n",
            " [0.52543586]\n",
            " [0.4696747 ]\n",
            " [0.7527712 ]\n",
            " [0.8001289 ]\n",
            " [0.5714184 ]\n",
            " [0.78665227]\n",
            " [0.48885882]\n",
            " [0.8181276 ]\n",
            " [0.50745   ]\n",
            " [0.7219303 ]\n",
            " [0.4526003 ]\n",
            " [0.7451917 ]\n",
            " [0.4633473 ]\n",
            " [0.50555605]\n",
            " [0.68466693]\n",
            " [0.7383519 ]\n",
            " [0.99668026]\n",
            " [0.635412  ]\n",
            " [0.45737764]\n",
            " [0.6506819 ]\n",
            " [0.6665562 ]\n",
            " [0.40037164]\n",
            " [0.75940007]\n",
            " [0.6820154 ]\n",
            " [0.7164261 ]\n",
            " [0.48245406]\n",
            " [0.6312427 ]\n",
            " [0.8405588 ]\n",
            " [0.7464195 ]\n",
            " [0.4503937 ]\n",
            " [0.6392538 ]\n",
            " [0.5722775 ]\n",
            " [0.5388871 ]\n",
            " [0.8268814 ]\n",
            " [0.4907875 ]\n",
            " [0.84402514]\n",
            " [0.8357488 ]\n",
            " [0.58521444]\n",
            " [0.738853  ]\n",
            " [0.6289709 ]\n",
            " [0.6616926 ]\n",
            " [0.48855338]\n",
            " [0.6691659 ]\n",
            " [0.46943316]\n",
            " [0.9745274 ]\n",
            " [0.6260073 ]\n",
            " [0.47554386]\n",
            " [0.8414243 ]\n",
            " [0.47694272]\n",
            " [0.54742396]\n",
            " [0.7102093 ]\n",
            " [0.6305723 ]\n",
            " [0.56384903]\n",
            " [0.82479835]\n",
            " [0.47167227]\n",
            " [0.6188063 ]\n",
            " [0.5663768 ]\n",
            " [0.5264464 ]\n",
            " [0.7510001 ]\n",
            " [0.5807327 ]\n",
            " [0.45351246]\n",
            " [0.5484959 ]\n",
            " [0.7873821 ]\n",
            " [0.639438  ]\n",
            " [0.695678  ]\n",
            " [0.59328884]\n",
            " [0.6228824 ]\n",
            " [0.8635254 ]\n",
            " [0.65361035]\n",
            " [0.59370816]\n",
            " [0.80837417]\n",
            " [0.9905946 ]\n",
            " [0.6068454 ]\n",
            " [0.6993216 ]\n",
            " [0.5893026 ]\n",
            " [0.48855338]\n",
            " [0.94719255]\n",
            " [0.48357666]\n",
            " [0.48855338]\n",
            " [0.7110141 ]\n",
            " [0.93427736]\n",
            " [0.5300734 ]\n",
            " [0.4713004 ]\n",
            " [0.906042  ]\n",
            " [0.681038  ]\n",
            " [0.6824235 ]\n",
            " [0.76812655]\n",
            " [0.7113237 ]\n",
            " [0.5199879 ]\n",
            " [0.8554476 ]\n",
            " [0.88879   ]\n",
            " [0.8538302 ]\n",
            " [0.49733648]\n",
            " [0.6499659 ]\n",
            " [0.73759425]\n",
            " [0.7283164 ]\n",
            " [0.4762065 ]\n",
            " [0.6952926 ]\n",
            " [0.63566405]\n",
            " [0.71561784]\n",
            " [0.5742484 ]\n",
            " [0.50973934]\n",
            " [0.48012534]\n",
            " [0.6669472 ]\n",
            " [0.48444706]\n",
            " [0.51295906]\n",
            " [0.5780037 ]\n",
            " [0.78529143]\n",
            " [0.6445268 ]\n",
            " [0.90465957]\n",
            " [0.6210225 ]\n",
            " [0.52581686]\n",
            " [0.6011424 ]\n",
            " [0.5598354 ]\n",
            " [0.49634477]\n",
            " [0.4807704 ]\n",
            " [0.6198552 ]\n",
            " [0.696015  ]\n",
            " [0.9132077 ]\n",
            " [0.82362217]\n",
            " [0.45819995]\n",
            " [0.6568569 ]\n",
            " [0.72942686]\n",
            " [0.7698587 ]\n",
            " [0.53405756]\n",
            " [0.5940999 ]\n",
            " [0.6077235 ]\n",
            " [0.58441293]\n",
            " [0.58979636]\n",
            " [0.6336842 ]\n",
            " [0.47405007]\n",
            " [0.62153906]\n",
            " [0.70846134]\n",
            " [0.6901312 ]\n",
            " [0.47165996]\n",
            " [0.5601309 ]\n",
            " [0.49899125]\n",
            " [0.47168338]\n",
            " [0.48855338]\n",
            " [0.48121715]\n",
            " [0.51818234]\n",
            " [0.66413057]\n",
            " [0.4541826 ]\n",
            " [0.57045764]\n",
            " [0.61382747]\n",
            " [0.48729363]\n",
            " [0.9216137 ]\n",
            " [0.586943  ]\n",
            " [0.7704273 ]\n",
            " [0.5845985 ]\n",
            " [0.76279175]\n",
            " [0.49574357]\n",
            " [0.98761356]\n",
            " [0.52940035]\n",
            " [0.5272391 ]\n",
            " [0.961739  ]\n",
            " [0.5851079 ]\n",
            " [0.49702033]\n",
            " [0.4811159 ]\n",
            " [0.89582735]\n",
            " [0.47194973]\n",
            " [0.65729463]\n",
            " [0.5691262 ]\n",
            " [0.815865  ]\n",
            " [0.481477  ]\n",
            " [0.9862331 ]\n",
            " [0.50237274]\n",
            " [0.52907556]\n",
            " [0.48383257]\n",
            " [0.90695226]\n",
            " [0.55638677]\n",
            " [0.82311577]\n",
            " [0.93803585]\n",
            " [0.6226194 ]\n",
            " [0.9895635 ]\n",
            " [0.61349356]\n",
            " [0.48501647]\n",
            " [0.7725769 ]\n",
            " [0.98734623]\n",
            " [0.85383016]\n",
            " [0.68761706]\n",
            " [0.6134234 ]\n",
            " [0.84230304]\n",
            " [0.5627119 ]\n",
            " [0.6856352 ]\n",
            " [0.67612565]\n",
            " [0.479522  ]\n",
            " [0.58764833]\n",
            " [0.85111177]\n",
            " [0.9424427 ]\n",
            " [0.55498403]\n",
            " [0.73378414]\n",
            " [0.48855338]\n",
            " [0.48009259]\n",
            " [0.7149281 ]\n",
            " [0.89017004]\n",
            " [0.4681088 ]\n",
            " [0.949694  ]\n",
            " [0.580809  ]\n",
            " [0.7146615 ]\n",
            " [0.5177082 ]\n",
            " [0.4729345 ]\n",
            " [0.6038237 ]\n",
            " [0.5209733 ]\n",
            " [0.70635176]\n",
            " [0.7578672 ]\n",
            " [0.13448913]\n",
            " [0.45671776]\n",
            " [0.5228789 ]\n",
            " [0.91756374]\n",
            " [0.5741624 ]\n",
            " [0.7666341 ]\n",
            " [0.6551592 ]\n",
            " [0.74490535]\n",
            " [0.6811849 ]\n",
            " [0.85757023]\n",
            " [0.9272096 ]\n",
            " [0.63656145]\n",
            " [0.47935176]\n",
            " [0.33276048]\n",
            " [0.4802346 ]\n",
            " [0.51889205]\n",
            " [0.72947055]\n",
            " [0.48925456]\n",
            " [0.9786518 ]\n",
            " [0.61613035]\n",
            " [0.55247855]\n",
            " [0.6896149 ]\n",
            " [0.71603084]\n",
            " [0.4821633 ]\n",
            " [0.46986842]\n",
            " [0.5789884 ]\n",
            " [0.5619241 ]\n",
            " [0.7581762 ]\n",
            " [0.74805504]\n",
            " [0.81422246]\n",
            " [0.8116685 ]\n",
            " [0.69229037]\n",
            " [0.6571756 ]\n",
            " [0.7590167 ]\n",
            " [0.5088918 ]\n",
            " [0.6261479 ]\n",
            " [0.8144123 ]\n",
            " [0.8878072 ]\n",
            " [0.6409104 ]\n",
            " [0.7853046 ]\n",
            " [0.4220744 ]\n",
            " [0.46494654]\n",
            " [0.8320825 ]\n",
            " [0.74821   ]\n",
            " [0.47130173]\n",
            " [0.4978055 ]\n",
            " [0.56703216]\n",
            " [0.47784266]\n",
            " [0.00633807]\n",
            " [0.6782305 ]\n",
            " [0.6387146 ]\n",
            " [0.54583985]\n",
            " [0.6915398 ]\n",
            " [0.7230792 ]\n",
            " [0.31908002]\n",
            " [0.8023344 ]\n",
            " [0.57598996]\n",
            " [0.66011506]\n",
            " [0.70751137]\n",
            " [0.5142326 ]\n",
            " [0.48855338]\n",
            " [0.6998209 ]\n",
            " [0.46436772]\n",
            " [0.48855338]\n",
            " [0.5949586 ]\n",
            " [0.69775885]\n",
            " [0.58919704]\n",
            " [0.87019646]\n",
            " [0.7903103 ]\n",
            " [0.7999251 ]\n",
            " [0.9388469 ]\n",
            " [0.5996782 ]\n",
            " [0.7646161 ]\n",
            " [0.6021328 ]\n",
            " [0.5393871 ]\n",
            " [0.5386929 ]\n",
            " [0.9556939 ]\n",
            " [0.80336714]\n",
            " [0.5699195 ]\n",
            " [0.9783865 ]\n",
            " [0.5744679 ]\n",
            " [0.78323   ]\n",
            " [0.5152608 ]\n",
            " [0.5920613 ]\n",
            " [0.9503181 ]\n",
            " [0.62257564]\n",
            " [0.77148277]\n",
            " [0.5927193 ]\n",
            " [0.4766338 ]\n",
            " [0.6527992 ]\n",
            " [0.4989604 ]\n",
            " [0.4552226 ]\n",
            " [0.60782796]\n",
            " [0.5307243 ]\n",
            " [0.5722748 ]\n",
            " [0.8344338 ]\n",
            " [0.5653541 ]\n",
            " [0.5423312 ]\n",
            " [0.48855338]\n",
            " [0.6392472 ]\n",
            " [0.58418113]\n",
            " [0.69631034]\n",
            " [0.73628134]\n",
            " [0.46830642]\n",
            " [0.469858  ]\n",
            " [0.5251423 ]\n",
            " [0.8201293 ]\n",
            " [0.5697381 ]\n",
            " [0.48855338]\n",
            " [0.8554606 ]\n",
            " [0.5605604 ]\n",
            " [0.95748866]\n",
            " [0.7528178 ]\n",
            " [0.6533942 ]\n",
            " [0.6148207 ]\n",
            " [0.67399645]\n",
            " [0.5143017 ]\n",
            " [0.5054375 ]\n",
            " [0.5154034 ]\n",
            " [0.63183206]\n",
            " [0.48836127]\n",
            " [0.51391894]\n",
            " [0.56331676]\n",
            " [0.48855338]\n",
            " [0.55921215]\n",
            " [0.47094828]\n",
            " [0.5173147 ]\n",
            " [0.5731868 ]\n",
            " [0.66512793]\n",
            " [0.48178178]\n",
            " [0.5427333 ]\n",
            " [0.6190323 ]\n",
            " [0.5898164 ]\n",
            " [0.52286065]\n",
            " [0.47509393]\n",
            " [0.4645815 ]\n",
            " [0.48194078]\n",
            " [0.6123196 ]\n",
            " [0.09616283]\n",
            " [0.7838089 ]\n",
            " [0.64733315]\n",
            " [0.8445787 ]\n",
            " [0.634652  ]\n",
            " [0.83244497]\n",
            " [0.5648405 ]\n",
            " [0.51269484]\n",
            " [0.57911134]\n",
            " [0.5132186 ]\n",
            " [0.46493736]\n",
            " [0.52919394]\n",
            " [0.67572093]]\n"
          ]
        }
      ],
      "source": [
        "#plot(dataset, probability)\n",
        "print(probabilities)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "whitefly_test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNQ0/tVcVZYAp4z8FjS9Qvg",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}